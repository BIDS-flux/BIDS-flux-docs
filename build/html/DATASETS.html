

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Datasets &mdash; Courtois NeuroMod 0.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/logo_neuromod_small.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Ethics" href="ETHICS.html" />
    <link rel="prev" title="Courtois NeuroMod" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Courtois NeuroMod
          

          
            
            <img src="_static/logo_neuromod_black.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hcp-trt">HCP-trt</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gambling">Gambling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#motor">Motor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#language-processing">Language processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#social-cognition">Social cognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relational-processing">Relational processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#emotion-processing">Emotion processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#working-memory">Working memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#resting-state">Resting state</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#movie-10">Movie-10</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ETHICS.html">Ethics</a></li>
<li class="toctree-l1"><a class="reference internal" href="PHYSIOLOGY.html">Physiology</a></li>
<li class="toctree-l1"><a class="reference internal" href="MRI.html">MRI</a></li>
<li class="toctree-l1"><a class="reference internal" href="DERIVATIVES.html">Derivatives</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Courtois NeuroMod</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Datasets</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/DATASETS.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="hcp-trt">
<h2>HCP-trt<a class="headerlink" href="#hcp-trt" title="Permalink to this headline">¶</a></h2>
<p>Four out of six participants (<code class="docutils literal notranslate"><span class="pre">P01-03</span></code>, <code class="docutils literal notranslate"><span class="pre">P05</span></code>) were scanned for a total of approximately 10 hours of functional data per subject. This <code class="docutils literal notranslate"><span class="pre">cneuromod</span></code> dataset is called HCP test-retest (<code class="docutils literal notranslate"><span class="pre">HCP-trt</span></code>), because participants repeated 15 times the functional localizers developed by the Human Connectome Project. The protocol consisted of seven tasks, described below (text adapted from the <a class="reference external" href="http://protocols.humanconnectome.org/HCP/3T/task-fMRI-protocol-details.html">HCP protocol site</a>). Before each task participants are given detailed instructions, and are given examples, as well as a practice run. A session was typically composed either of two repetitions of the HCP localizers, or of one resting-state run, and one repetition of HCP localizers.</p>
<p>For more information check out about the tasks, check out the [Human Connectome Project] (http://protocols.humanconnectome.org/HCP/3T/task-fMRI-protocol-details.html). Stimuli and e-prime scripst were were provided by the Human Connectome Project, U-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.</p>
<p>The dataset is accessible in datalad <a class="reference external" href="https://git.unf-montreal.ca/neuromod/phantom_hcptrt">here</a> (access request needed).
The eprime scripts for preparation and presentation of the stimuli can be found in the HCP database: <a class="reference external" href="https://db.humanconnectome.org/app/action/ChooseDownloadResources?project=HCP_Resources&amp;resource=Scripts&amp;filePath=HCP_TFMRI_scripts.zip">link to eprime scripts and stimuli</a>.</p>
<div class="section" id="gambling">
<h3>Gambling<a class="headerlink" href="#gambling" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 4 minutes. Participants played a card guessing game where they were told to guess whether the number of a card (represented by a “?” presented for 1500ms on the screen) was above or below 5 (Delgado et al. 2000). They indicated their choice using button press, and following their choice they were shown the correct number. If they guessed correctly they were told they won money ($1.00 - reward condition), if they guessed incorrectly they were told they lost money ($0.50 - loss condition), and if the number was exactly 5 they were told that they neither won or lost money (neutral condition). Note that no money was actually given to the participants and, as such, this task may not be valid but was still included to accurately reproduce the HCP protocol. The conditions were presented in blocks of 8 trials that were either mostly reward (6 reward trials pseudo randomly interleaved with either 1 neutral and 1 loss trial, 2 neutral trials, or 2 loss trials) or mostly loss (6 loss trials pseudo-randomly interleaved with either 1 neutral and 1 reward trial, 2 neutral trials, or 2 reward trials). There were four blocks per run (2 mostly win and 2 mostly loss), and two runs in total.</p>
</div>
<div class="section" id="motor">
<h3>Motor<a class="headerlink" href="#motor" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 5 minutes. This task was adapted from (Buckner et al. 2011; Yeo et al. 2011). Participants were presented a visual cue, and were asked to either tap their left or right fingers, squeeze their left or right toes, or move their tongue to map motor area. Each movement lasted 12 seconds, and in total there were 13 blocks, with 2 of tongue movements, 4 of hand movements (2 right and 2 left), and 4 of foot movements (2 right and 2 left), and three 15 second fixation blocks where participants were instructed not to move anything. There were two runs in total, and 13 blocks per run.</p>
</div>
<div class="section" id="language-processing">
<h3>Language processing<a class="headerlink" href="#language-processing" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 5 minutes. Participants either listened to an auditory story (5-9 sentences, about 20 seconds), followed by a two-alternative forced-choice question, or they listened to a math problem (addition and subtraction only, varies in length), and were instructed to push a button to select the first or the second answer as being correct. The task was adaptive so that for every correct answer the level of difficulty increased. The math task was designed this way to maintain the same level of difficulty between participants. There were 2 runs, each with 4 story and 4 math blocks, interleaved.</p>
</div>
<div class="section" id="social-cognition">
<h3>Social cognition<a class="headerlink" href="#social-cognition" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 5 minutes. Participants were presented with short video clips (20 seconds) of objects (squares, circles, triangles) that either interacted in some way, or moved randomly on the screen (Castelli et al. 2000; (Wheatley et al. 2007). Following each clip, participants were asked to judge whether the objects had a “Mental interaction” (an interaction that appeared as if the shapes were taking into account each other’s feelings and thoughts), whether the were “Not Sure”, or if there was “No interaction”. Button presses were used to record their responses. In each of the two runs, participants viewed 5 “Mental” videos and 5 random videos and had 5 fixation blocks of 15 seconds each.</p>
</div>
<div class="section" id="relational-processing">
<h3>Relational processing<a class="headerlink" href="#relational-processing" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 5 minutes. Participants were shown 6 different shapes filled with 1 of 6 different textures (Smith et al. 2007). There were two conditions: relations processing, and control matching condition. In the relational processing condition, 2 pairs of objects were presented on the screen, with one pair at the top of the screen, and the other pair at the bottom. Participants were instructed to decide what dimension differed in the top pair (shape or texture), and then decide if the bottom pair differed, or not, on the same dimension (i.e. if the top pair differed in shape, did the bottom pair also differ in shape). Their answers were recorded by one of two button presses: “a” differ on same dimension; “b” don’t differ on same dimension. In the control matching condition, participants were shown two objects at the top of the screen, and one object at the bottom of the screen, with a word in the middle of the screen (either “shape” or “texture”).They were told to decide whether the bottom object matched either of the top two objects on that dimension (i.e., if the word is “shape”, did the bottom object have the same shape as either of the top two objects). Participants responded “yes” or “no” using the button box. For the relational condition, the stimuli were presented for 3500 ms, with a 500 ms ITI, and there were four trials per block. In the matching condition, stimuli were presented for 2800 ms, with a 400 ms ITI, and there were 5 trials per block. In total there were two runs, each with three relational blocks, three matching blocks and three 16-second fixation blocks</p>
</div>
<div class="section" id="emotion-processing">
<h3>Emotion processing<a class="headerlink" href="#emotion-processing" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 4 minutes. Participants were shown triads of faces or shapes, and were asked to decide which of the shapes at the bottom of the screen matches the target face/ shape at the top of the screen (adapted from Smith et al. 2007). Faces had either an angry or fearful expression. Faces, and shapes were presented in three blocks of 6 trials (3 faces and 3 shapes), with each trial lasting 2 seconds, followed by a 1 second inter-stimulus interval. Each block was preceded by a 3000 ms task cue (“shape” or “face”), so that each block was 21 seconds long, including the cue. In total there were two runs, three face blocks and three shape blocks, with 8 seconds of fixation at the end of each run.</p>
</div>
<div class="section" id="working-memory">
<h3>Working memory<a class="headerlink" href="#working-memory" title="Permalink to this headline">¶</a></h3>
<p>Duration: approximately 5 minutes. There were two subtasks: a category specific representation, and a working memory task. Participants were presented with blocks of either places, tools, faces, and body parts. Within each run, all 4 types of stimuli were presented in block, with each block being labelled as a 2-back task (participants needed to indicate if they saw the same image two images back), or a version of a 0-back task (participants were shown a target at the start of the trial and they needed to indicate if the image that they were seeing matched the target). Each image was presented for 2 seconds, followed by a 500 ms ITI. Stimuli were presented for 2 seconds, followed by a 500 ms inter-task interval. Each of the 2 runs included 10 trials, and 4 fixations blocks (15 secs).</p>
</div>
<div class="section" id="resting-state">
<h3>Resting state<a class="headerlink" href="#resting-state" title="Permalink to this headline">¶</a></h3>
<p>Duration: 15 minutes. In every other session, one resting-state fMRI run was acquired. Participants were asked to have their eye open, be looking at fixation cross in the middle of the screen and be instructed to not fall asleep. A total of five resting-state fMRI runs were acquired per subject.</p>
</div>
</div>
<div class="section" id="movie-10">
<h2>Movie-10<a class="headerlink" href="#movie-10" title="Permalink to this headline">¶</a></h2>
<p>This dataset includes about 10 hours of functional data for all 6 participants (<code class="docutils literal notranslate"><span class="pre">P01-P06</span></code>). The participants watched the following movies:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/The_Bourne_Supremacy_%28film%29">The Bourne supremacy</a>. Duration ~100 minutes.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/The_Wolf_of_Wall_Street_%282013_film%29">The wolf of wall street</a>. Duration ~170 minutes.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden figures</a>. Duration ~120 minutes. This movie was presented twice, for a total duration of ~240 minutes.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Life_(British_TV_series)">Life</a> episode 2: “Reptiles and Amphibians”.  Duration ~XX minutes. This movie was presented twice, for a total duration of ~XX minutes.</p></li>
</ul>
<p>Each movie was cut into a series of ~10 minutes clips. Exact cutting points were manually selected to not interrupt the narrative flow. Fade out to a black screen was added at the end of each clip, and with a few seconds overlap between the end of a clip and the beginning of the next clip. The number of clips presented in a given session varied, and some subjects took short breaks inside a session.</p>
<p>The dataset is accessible in datalad <a class="reference external" href="https://git.unf-montreal.ca/neuromod/phantom_video">here</a> (access request needed).
The python &amp; psychopy scripts for preparation and presentation of the clips can be found in the following github <a class="reference external" href="https://github.com/courtois-neuromod/task_stimuli">repository</a>.</p>
<p>Bold scans are named as: <code class="docutils literal notranslate"><span class="pre">func_sub-&lt;participant&gt;_ses-&lt;session&gt;_task-&lt;movie&gt;_run-&lt;seg&gt;</span></code></p>
<p>Stimuli can be found in: <code class="docutils literal notranslate"><span class="pre">stimuli/&lt;movie&gt;/&lt;movie&gt;_seg&lt;seg&gt;.mkv</span></code></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ETHICS.html" class="btn btn-neutral float-right" title="Ethics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Courtois NeuroMod" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Courtois NeuroMod team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>